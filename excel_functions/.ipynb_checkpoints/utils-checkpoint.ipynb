{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Sequence\n",
    "from pathlib import Path\n",
    "import os\n",
    "import functools \n",
    "from shutil import copyfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('/home/chens/practicum/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. filter records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_records(df,treatment_range=None,date_range=None,primary_use_list=None,age_range=None,met_range=None,return_list = False,max_volume = None,min_volume = None) -> Sequence[str]:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    filter the medical records based on \n",
    "        1. the number of treatments\n",
    "        2. first treatment date\n",
    "        3. the primary use, age and number of mets on the first treatment\n",
    "    \n",
    "    return a list of filenames if the parameter 'return_list' is True, otherwise return a DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Treatment count\n",
    "    droped_duplicates_date = df[['PiCare PatientID','StudyDateAnon']].drop_duplicates().dropna()\n",
    "    treat_count = droped_duplicates_date.groupby(['PiCare PatientID']).agg({'StudyDateAnon':'count'}).reset_index().rename(columns={'StudyDateAnon':'Treatment Count'})\n",
    "    print(treat_count[['PiCare PatientID']].drop_duplicates().shape)\n",
    "    filtered_treatment_num = treat_count[treat_count['Treatment Count'].between(treatment_range[0],treatment_range[1])]\n",
    "    filtered_df = filtered_treatment_num.merge(df,on='PiCare PatientID',how='left')\n",
    "    print(filtered_df[['PiCare PatientID']].drop_duplicates().shape)\n",
    "    \n",
    "    # First Treatment Date\n",
    "    first_treatment_date = filtered_df.groupby(['PiCare PatientID']).agg({'StudyDateAnon':'min'}).reset_index()\n",
    "    filtered_date = first_treatment_date[first_treatment_date['StudyDateAnon'].between(date_range[0],date_range[1])]\n",
    "    filtered_df = filtered_date.merge(filtered_df,on=['PiCare PatientID','StudyDateAnon'],how='left')[['PiCare PatientID',\n",
    "                                                                                          'StudyDateAnon',\n",
    "                                                                                          'Primary tumor Site',\n",
    "                                                                                          'Age primary diag','Target volume']].drop_duplicates()\n",
    "    # Max sizes\n",
    "    max_target_volume = filtered_df.groupby(['PiCare PatientID','StudyDateAnon']).agg({'Target volume':'max'}).reset_index()\n",
    "    filtered_df = filtered_df.merge(max_target_volume[(max_target_volume['Target volume']>=min_volume)&(max_target_volume['Target volume']<max_volume)][['PiCare PatientID']],on = 'PiCare PatientID',how = 'right')\n",
    "    print(filtered_df[['PiCare PatientID']].drop_duplicates().shape)\n",
    "    # Primary Use on First Treatment\n",
    "    filtered_df = filtered_df[filtered_df['Primary tumor Site'].isin(primary_use_list)]\n",
    "    print(filtered_df[['PiCare PatientID']].drop_duplicates().shape)\n",
    "    # Age on First Treatment \n",
    "    filtered_df = filtered_df[filtered_df['Age primary diag'].between(age_range[0],age_range[1])]\n",
    "    print(filtered_df[['PiCare PatientID']].drop_duplicates().shape)\n",
    "    # Number of mets on First Treatment\n",
    "    num_of_mets = filtered_df.merge(df,on=['PiCare PatientID','StudyDateAnon','Primary tumor Site','Age primary diag'],how='left').assign(const=1).groupby(['PiCare PatientID','StudyDateAnon']).agg({'const':'count'}).rename(columns = {'const':'num of mets'}).reset_index()\n",
    "    filtered_mets = num_of_mets[num_of_mets['num of mets'].between(met_range[0],met_range[1])]\n",
    "    filtered_df = filtered_mets.merge(filtered_df,on = ['PiCare PatientID','StudyDateAnon'],how='left').dropna()\n",
    "    print(filtered_df[['PiCare PatientID']].drop_duplicates().shape)\n",
    "    if return_list:\n",
    "        return list(set([\n",
    "                str(filtered_df.iloc[i,0])\n",
    "                +'_'\n",
    "                +''.join(str(filtered_df.iloc[i,1]).split(' ')[0].split('-'))\n",
    "                for i in range(len(filtered_df))\n",
    "        ]))\n",
    "    else: \n",
    "        return filtered_df.rename(columns={'StudyDateAnon':'First Study Date'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/data/public/MIM_BMETS_V6/3_final_datasets/manuscript_1_datasets/Master_BrainMets_List_Anon_June.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['StudyDateAnon'] = df['StudyDateAnon'].replace(to_replace=None, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(1790, 1)\n",
      "(1790, 1)\n",
      "(1730, 1)\n",
      "(1730, 1)\n"
     ]
    }
   ],
   "source": [
    "selected_df = filter_records(\n",
    "    df,\n",
    "    treatment_range = [1,100000],\n",
    "    date_range = ['1700-01-01','2020-03-01'],\n",
    "    primary_use_list = list(df['Primary tumor Site'].unique()),\n",
    "    met_range = [1,1000000],\n",
    "    age_range=[1,100],\n",
    "    return_list = False,\n",
    "    min_volume = 0.,\n",
    "    max_volume = 1000000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "6094    False\n",
       "6095    False\n",
       "6096    False\n",
       "6097    False\n",
       "6098    False\n",
       "Name: num of mets, Length: 6080, dtype: bool"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df['num of mets']==2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1met_dont_include.xlsx',\n",
       " '3mets-500',\n",
       " '11+mets-500',\n",
       " 'first_tx_allmets_0-0.5cc',\n",
       " 'first_tx_4-5mets',\n",
       " 'first_tx_2mets',\n",
       " '6-10mets_dont_include.xlsx',\n",
       " 'first_tx_3mets',\n",
       " 'first_tx_6-10mets',\n",
       " '6-10mets-500',\n",
       " '4-5mets_dont_include.xlsx',\n",
       " 'Master_BrainMets_List_Anon_June.xlsx',\n",
       " '4-5mets-500',\n",
       " '3mets_dont_include.xlsx',\n",
       " 'first_tx_allmets',\n",
       " 'first_tx_1met',\n",
       " 'first_tx_11+mets',\n",
       " '11+mets_dont_include.xlsx',\n",
       " '1met-500',\n",
       " '2mets_dont_include.xlsx',\n",
       " '2mets-500']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path('/data/public/MIM_BMETS_V6/3_final_datasets/manuscript_1_datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_image_mask_existence(file_path,name_list_from_excel):\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     This function is for checking existence of real images based on the Patient_ID list created by \n",
    "#     filter_records function above\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     namelists = [path[2] for path in list(os.walk(file_path))[1:]]\n",
    "#     namesets = []\n",
    "#     for namelist in namelists:\n",
    "#         namesets.append(set(['_'.join(name.split('_')[:2]) for name in namelist]))\n",
    "    \n",
    "#     #brain_masks_set, mets_masks, images, skll_stripped = namesets\n",
    "#     existing_name_list = functools.reduce(lambda x,y:x.intersection(y),namesets)\n",
    "#     result = [name for name in name_list_from_excel if name in existing_name_list]\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_image_mask_existence_for_certain_volume(file_path,name_list_from_excel):\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     This function is for checking existence of real images based on the Patient_ID list created by \n",
    "#     filter_records function above\n",
    "    \n",
    "#     \"\"\"\n",
    "#     sub_folders= ['first_tx_1met','first_tx_2mets','first_tx_3mets','first_tx_4-5mets','first_tx_6-10mets','first_tx_11+mets']\n",
    "#     namelists = []\n",
    "#     for sub_folder in sub_folders:\n",
    "#         namelists += [path[2] for path in list(os.walk(file_path/sub_folder))[1:] if len(path[2])>0]\n",
    "#     namelist = functools.reduce(lambda x,y:x+y,namelists)\n",
    "#     nameset = set(namelist)\n",
    "#     print(len(nameset))\n",
    "#     #brain_masks_set, mets_masks, images, skll_stripped = namesets\n",
    "#     existing_name_list = list(nameset)\n",
    "#     result = [name for name in name_list_from_excel if name+'.npy' in existing_name_list]\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477\n"
     ]
    }
   ],
   "source": [
    "# file_path = Path('/data/public/MIM_BMETS_V6/3_final_datasets/manuscript_1_datasets')\n",
    "# # sub_sets = ['training','validation','testing']\n",
    "# sub_sets = ['training']\n",
    "# total_num=0\n",
    "# existing_file = []\n",
    "# for sub_folder in ['first_tx_allmets']:\n",
    "#     for sub_set in sub_sets:\n",
    "#         total_num+=len(os.listdir(file_path/sub_folder/sub_set/'skull_stripped_1x1x3'))\n",
    "#         existing_file+=[str(file_path/sub_folder/sub_set/'skull_stripped_1x1x3')+'/'+name for name in os.listdir(file_path/sub_folder/sub_set/'skull_stripped_1x1x3')]\n",
    "#         print(len(os.listdir(file_path/sub_folder/sub_set/'skull_stripped_1x1x3')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "existing_file_list_size = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(310, 1)\n",
      "(310, 1)\n",
      "(294, 1)\n",
      "(294, 1)\n",
      "477\n",
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(187, 1)\n",
      "(187, 1)\n",
      "(180, 1)\n",
      "(180, 1)\n",
      "477\n",
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(424, 1)\n",
      "(424, 1)\n",
      "(413, 1)\n",
      "(413, 1)\n",
      "477\n",
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(293, 1)\n",
      "(293, 1)\n",
      "(285, 1)\n",
      "(285, 1)\n",
      "477\n",
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(354, 1)\n",
      "(354, 1)\n",
      "(340, 1)\n",
      "(340, 1)\n",
      "477\n",
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(222, 1)\n",
      "(222, 1)\n",
      "(218, 1)\n",
      "(218, 1)\n",
      "477\n"
     ]
    }
   ],
   "source": [
    "for size, (min_volume,max_volume) in enumerate([(0.,0.499),(0.5,0.999),(1.,2.999),(3.,4.999),(5.,9.999),(10.,10000.)]):\n",
    "    selected_df = filter_records(\n",
    "        df,\n",
    "        treatment_range = [1,100000],\n",
    "        date_range = ['1700-01-01','2020-03-01'],\n",
    "        primary_use_list = list(df['Primary tumor Site'].unique()),\n",
    "        met_range = [1,1000000],\n",
    "        age_range=[1,100],\n",
    "        return_list = True,\n",
    "        min_volume = min_volume,\n",
    "        max_volume = max_volume\n",
    "        )\n",
    "    file_path = Path('/data/public/MIM_BMETS_V6/3_final_datasets/manuscript_1_datasets')\n",
    "    sub_sets = ['training']\n",
    "    total_num=0\n",
    "    existing_file = []\n",
    "    for sub_folder in ['first_tx_allmets']:\n",
    "        for sub_set in sub_sets:\n",
    "            total_num+=len(os.listdir(file_path/sub_folder/sub_set/'skull_stripped_1x1x3'))\n",
    "            existing_file+=[str(file_path/sub_folder/sub_set/'skull_stripped_1x1x3')+'/'+name for name in os.listdir(file_path/sub_folder/sub_set/'skull_stripped_1x1x3')]\n",
    "            print(len(os.listdir(file_path/sub_folder/sub_set/'skull_stripped_1x1x3')))\n",
    "    existing_file_list_size += [i for i in existing_file if i.split('/')[-1] in [j+'.npy' for j in selected_df]]\n",
    "    sizes += [size for i in range(len([i for i in existing_file if i.split('/')[-1] in [j+'.npy' for j in selected_df]]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(existing_file_list_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5]), array([ 45,  46, 116,  89, 103,  61]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sizes,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = []\n",
    "existing_file_list_num = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(1790, 1)\n",
      "(1790, 1)\n",
      "(1730, 1)\n",
      "(497, 1)\n",
      "477\n",
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(1790, 1)\n",
      "(1790, 1)\n",
      "(1730, 1)\n",
      "(2, 1)\n",
      "477\n",
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(1790, 1)\n",
      "(1790, 1)\n",
      "(1730, 1)\n",
      "(0, 1)\n",
      "477\n",
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(1790, 1)\n",
      "(1790, 1)\n",
      "(1730, 1)\n",
      "(344, 1)\n",
      "477\n",
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(1790, 1)\n",
      "(1790, 1)\n",
      "(1730, 1)\n",
      "(244, 1)\n",
      "477\n",
      "(1866, 1)\n",
      "(1866, 1)\n",
      "(1790, 1)\n",
      "(1790, 1)\n",
      "(1730, 1)\n",
      "(643, 1)\n",
      "477\n"
     ]
    }
   ],
   "source": [
    "for num, (min_num,max_num) in enumerate([(0.5,1.5),(1.5,2.5),(2.5,3.5),(3.5,5.5),(5.5,10.5),(10.5,10000)]):\n",
    "    selected_df = filter_records(\n",
    "        df,\n",
    "        treatment_range = [1,100000],\n",
    "        date_range = ['1700-01-01','2020-03-01'],\n",
    "        primary_use_list = list(df['Primary tumor Site'].unique()),\n",
    "        met_range = [min_num,max_num],\n",
    "        age_range=[1,100],\n",
    "        return_list = True,\n",
    "        min_volume = -1,\n",
    "        max_volume = 1000000\n",
    "        )\n",
    "    file_path = Path('/data/public/MIM_BMETS_V6/3_final_datasets/manuscript_1_datasets')\n",
    "    sub_sets = ['training']\n",
    "    total_num=0\n",
    "    existing_file = []\n",
    "    for sub_folder in ['first_tx_allmets']:\n",
    "        for sub_set in sub_sets:\n",
    "            total_num+=len(os.listdir(file_path/sub_folder/sub_set/'skull_stripped_1x1x3'))\n",
    "            existing_file+=[str(file_path/sub_folder/sub_set/'skull_stripped_1x1x3')+'/'+name for name in os.listdir(file_path/sub_folder/sub_set/'skull_stripped_1x1x3')]\n",
    "            print(len(os.listdir(file_path/sub_folder/sub_set/'skull_stripped_1x1x3')))\n",
    "    existing_file_list += [i for i in existing_file if i.split('/')[-1] in [j+'.npy' for j in selected_df]]\n",
    "    nums += [num for i in range(len([i for i in existing_file if i.split('/')[-1] in [j+'.npy' for j in selected_df]]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 3, 4, 5]), array([118,  97,  57, 188]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(nums,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['PiCare PatientID'] == 'BrainMets-UCSF-00147']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing_name_list = filter_image_mask_existence_for_certain_volume(file_path,selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(existing_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing_name_list2 = filter_image_mask_existence_for_certain_volume(file_path,selected_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(existing_name_list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_folders(target_folder):\n",
    "    \"\"\"\n",
    "    \n",
    "    Spliting the data filtered above into train, validation and test sets.\n",
    "    Creating a folder to store this subset.\n",
    "    \n",
    "    \"\"\"\n",
    "    main_path = Path('/home/chens/practicum/MEDomicsLab-develop-brainmets/image_processing/manuscript_1_datasets')\n",
    "    target_folder = main_path/target_folder\n",
    "    target_folder.mkdir()\n",
    "    for name in ['training','validation','testing']:\n",
    "        (target_folder/name).mkdir()\n",
    "        for filetype in ['brain_masks_1x1x3','images_1x1x3','mets_masks_1x1x3','skull_stripped_1x1x3']:\n",
    "            (target_folder/name/filetype).mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_spliting(filelist,train_size,validation_size):\n",
    "    train,validation_test = train_test_split(filelist,train_size=train_size,shuffle=True)\n",
    "    validation,test = train_test_split(validation_test,train_size=validation_size/(1-train_size),shuffle=True)\n",
    "#     print(validation_size/(1-train_size))\n",
    "    return train,validation,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_subsets(target_folder,train,validation,test):\n",
    "    original_path = Path('/data/public/MIM_BMETS_V6/3_final_datasets/manuscript_1_datasets/first_tx_allmets')\n",
    "    main_path = Path('/home/chens/practicum/MEDomicsLab-develop-brainmets/image_processing/manuscript_1_datasets')\n",
    "    target_folder = main_path/target_folder\n",
    "#     for name in ['training','validation','testing']:\n",
    "    for filetype in ['brain_masks_1x1x3','images_1x1x3','mets_masks_1x1x3','skull_stripped_1x1x3']:\n",
    "        for file in tqdm(existing_file_list):\n",
    "            if file in train:\n",
    "                copyfile(file.replace('skull_stripped_1x1x3',filetype),target_folder/'training'/filetype/file.split('/')[-1])\n",
    "            elif file in validation:\n",
    "                copyfile(file.replace('skull_stripped_1x1x3',filetype),target_folder/'validation'/filetype/file.split('/')[-1])\n",
    "            elif file in test:\n",
    "                copyfile(file.replace('skull_stripped_1x1x3',filetype),target_folder/'testing'/filetype/file.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_folder='first_tx_allmets_0-0.5cc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "creating_folders(target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,validation,test = data_spliting(existing_file_list,0.7,0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "51\n",
      "10\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(existing_file_list))\n",
    "print(len(train))\n",
    "print(len(validation))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:02<00:00, 26.97it/s]\n",
      "100%|██████████| 73/73 [00:02<00:00, 27.80it/s]\n",
      "100%|██████████| 73/73 [00:02<00:00, 28.99it/s]\n",
      "100%|██████████| 73/73 [00:02<00:00, 28.91it/s]\n"
     ]
    }
   ],
   "source": [
    "saving_subsets(target_folder,train,validation,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "80+65+186+117+144+81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
